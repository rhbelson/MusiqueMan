<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Ben Krege, Victor Lalo, Robbie Belson">

    <title>MusiqueMan</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <script src="index.js"></script>
    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/creative.min.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top">

<!-- <embed id="mus"
    src="homepage.mp3"
    loop="true"
    width="1"
    height="1"
    autostart="true"> -->

    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">MusiqueMan</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="#about">Project Overview</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#services">Approach and Design Considerations</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#portfolio">Our Music and Data</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <header>
        <div class="header-content">
            <div class="header-content-inner">
                <h1 id="homeHeading">Automated Music Generator using Recurrent Neural Network </h1>
                <hr>
                <p>EECS 349 Final Project <br>Ben Krege, Victor Lalo, and Robert Belson<br>
                    JohnKrege2018@u.northwestern.edu<br>
                    VictorLalo2017@u.northwestern.edu<br>
                    RobertBelson2019@u.northwestern.edu </p>
                <a href="#about" class="btn btn-primary btn-xl page-scroll">Learn More</a>
            </div>
        </div>
    </header>

    <section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Abstract</h2>
                    <hr class="light">
                    <p class="text-faded">Our task is to create a machine composer that learn composer and genre style, and  produce original work in that style. This task is important because we can experiment with different genres or mixes of input data to create new styles and compositions that can inspire our own work, or possibly be quality compositions in their own right. Bach was only able to write 500 chorals in his lifetime. Wouldn’t it be great if we could do this in an hour?  Furthermore, by interacting with a machine composer, writers can be provided material and inspiration for their work.
                        <br><br>
                    Using Daniel Johnson's Biaxial Long Short-Term Memory Biaxial Recurrent Neural Network, we generated new compositions of music in a variety of styles, from Classical (e.g., music in the style of Fredyryk Chopin) to Classic Rock (e.g., Music in the style of beatles). This neural network learner's strengths included high quality outputs – as music was aesthetically pleasing and aurally resembled the genre of the training set – and comprehensive feature set, as pattern recognition, meter, pitch-class were some of the features used. Given the complexity of the learner required to perform automated music composition coupled with an in-depth analysis of Daniel Johnson's algorithm itself, we concluded that a neural network is the only learner capable of producing meaningful outputs in this discipline. Furthermore, in experimenting with hyperparamter adjustment – such as dropout and temeprature – we determined that the adjustments that led to the most aurally recognizable changes resided in adjusting the number of nodes of hidden layers across the time and note axes.
</p>
                    <a href="#services" class="page-scroll btn btn-default btn-xl sr-button">How'd we do it?</a>
                </div>
            </div>
        </div>
    </section>

    <section id="services">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Project Design</h2>
                    <hr class="primary">
                </div>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-diamond text-primary sr-icons"></i>
                        <h3>Our Architecture: Daniel Johnson's Biaxial Long Short-Term Memory Recurrent Neural Network</h3>
                        <p class="text-muted">After just ~12 hours of training, producing a new piece takes just a matter of minutes. </p>
                    </div>
                </div>
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-paper-plane text-primary sr-icons"></i>
                        <h3>Endless, Recyclable Music Composition </h3>
                        <p class="text-muted">Using AWS to optimize performance, as the RNN was designed for GPUs, we queued up a variety of compositions over the course of a few weeks.</p>
                    </div>
                </div>
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-newspaper-o text-primary sr-icons"></i>
                        <h3>Our Data (MIDI Files)</h3>
                        <p class="text-muted">We have found numerous online MIDI libraries with raw data from a multitude of styles and composers. So far, we have used a dataset of a variety of works by Bach (chorales, fugues, variations, etc.) of size ~1.2MB (Complete Bach Midi Index from bachcentral.com). The 5 features include input position, pitch­ class, previous vicinity, previous context, and beat. As for examples, the examples are essential the repertoire itself, in which we will run out model on held out test data. Of the 1.2MB dataset we chose, all of it will be used for development/training, as a smaller partition may lead to an insufficient amount of data for automated music composition. </p>
                    </div>
                </div>
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-heart text-primary sr-icons"></i>
                        <h3>Breadth of Project</h3>
                        <p class="text-muted">Unlike Daniel Johnson, we sought to apply his neural network across a variety of styles, comparing its capcity to produce aurally compelling music. Beginning first with the classical genre, drawing from the compositions of Bach and Chopin, we then explored other genres such as classical rock, pop, and electronic and house music.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>



<section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">How does the Biaxial Recurrent Neural Network Work?</h2>
                    <hr class="light">
                    <p>Drawing from some concepts of convolution neural networks (applying the same invarient network on each time step)coupled with long short-term memory layers, our recurrent neural network learns to predict which notes will be played (among other paramters such as duration) at each time step (1/16th notes) of a piece.<br>

                    In basic recurrent neural networks, each output of a particular hidden layer, beyond the weighted sum of the inputs, is computed and fed back to itself, and other nodes in that "column," as an additional input in the next time step. Thus, each node of a hidden layer is derived by the outputs of the current layer and the inputs of the previous layer, as seen in the figure below:
                    <br>
                    <img src="img/rnn1.jpg" width="30%" align="middle" alt="">
                    <br>

                     For the network to have short-term-memory, an output needs to be an input in the next time step. Long Short-Term Memory (LSTM) nodes includes a saved value that is included in each node's calculations. This value can be modified (e.g., addeded, subtracted, etc.) at each time step.
                <br>
                <img src="img/rnn2.jpg" width="30%" align="middle" alt="">
                <br>
                    For this neural network adapted specifically to generate pieces of music, unique properties of this network included:
                <ol class="text-faded">
                  <li align="left">Note and Time Invariance: For the generated music to be transposable and composed to an indefinite length, the network was constructed to be identical for each note and time step (time+note invarient)</li>
                  <li align="left">Vertical and Horizontal Motion: Network supports polyphonic music, with multiple notes at a given time step</li>
                  <li align="left">Repetition as Integral Compositional Element: Western music is repetitive. Thus, the network must allow repetition of the same note. The uniqueness of this network is that it has an encoding for a note to be held. This contrasts some previous machine composer designs that did not distinguish between a held note and a repeated note.</li>
                </ol>
                However, to integrate pattern recognition over time (an integral component to constructing aurally). Coined as "Biaxial RNN" by Daniel Johnson, the network has time axis and note axis, connecting inputs to outputs and recurrent connections from one axis to another.

                REWRITE

                The first two layers (defaults to 300 nodes each) feed back (a la RNN) through time. the third and fourth timesteps (200 (text Ben and make sure its 200) and 50) feed back into themselves to give an awareness of harmony.

                <br>
                <br>
                <a id="rnncontbutt" class="page-scroll btn btn-default btn-xl sr-button">Implementation Details (Cont.)</a> 
</p>
                </div>
            </div>
        </div>
    </section>




<section hidden class="bg-primary" id="rnn_cont">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Deep Dive into Our Biaxial Recurrent Neural Network</h2>
                    <hr class="light">
                    <p align="left">For each time step we have the following paramters to our input vector:
                    <ol class="text-faded">
                  <li align="left">Position: Current pitch of a particular note, ranging from 0-127 (MIDI note value) </li>
                  <li align="left">Pitchlass: Consisting of 12 values, one per pitchclass, incrementing by 1 for each value (A will be 0, A# will be 1.... G# will be 11). In this assumption, Johnson posits that this will allow selection of "more common chords." However, this assumption is incredibly naive, given his assertion that is is necessarily "more common to have a C major chord than E-flat major chord." In certain iterations of our model, we adjusted this paramter to reflect a different weighting of commonality of various chords. </li>
                  <li align="left">Previous Vicinity: Consisting of 50 values, or rather 3 octaves span of 12 pitch classes, each value is given a 1 if that pitch was present in the last timestep and a zero otherwise. </li>
                  <li align="left">Previous Context: Similar to Previous Vicinity paramter, but condensed to 1 octave, number of times a certain pitch class was played across any octave is reported across the 12 values.</li>
                  <li align="left">Beat: Using a 4 row binary representation, one for each beat of the 4/4 time signature, each 1 represents the "note-on" MIDI property for each timestep.</li>
                </ol>
                <p align="left" >Now, to break down the two LSTM stacks:<br>
                First Hidden LSTM Stack: LSTMs that have reccurent connections along time axis, where the last layer outputs a note state representing any particular time pattern</p> <br>
                <p align="left" >Second Hidden LSTM Stack: LSTMs that have recurrent connections along note axis, whos last layer outputs 2 values. The first value, Play Probability, indicates the probability that a particular note should be chosen to be played. The second value, Articulate Probability (applies only for notes that have already been played in prior time step), is probability the note should last longer, say, from a sixteenth note to an eighth note. </p>
                <h2 class="section-heading">Model Software Implementation </h2>
                <p align="left" >Having studied a variety of machine learning software this quarter, the biaxial network uses Theano, a python library developed by computer scientists at Université de Montréal. This software compiles the network to run most efficiently by making the code GPU-optimized.
                <br>
                
                </p>
                <ol> Training Process Stepwise Process:
                  <li align="left">Randomly select batch of short music segments from our dataset, and feed into biaxial RNN</li>
                  <li align="left">Take output probabilities from second hidden LSTM stack, and calculate cross-entropy (i.e., likelihood of generating particular output) </li>
                  <li align="left">Take probabilities and use optimizer (AdaDelta) to calculate weights </li>
                  <li align="left">Batch all notes together and train time-axis layers</li>
                  <li align="left">Do the reverse: batch all times together and train the note-axis layers</li>
                </ol>
                <h2 class="section-heading">Avoiding Overfitting: Hyperparameter Adjustments </h2>
                <p align="left">Applying dropout to our RNN is a way of removing hidden nodes (by zeroing output) at each layer at random, as a means to avoid overfitting and in turn promote specializing, or rather "unique" music composition. <br>
                *Note: We would expect temperature adjustments to behave in a similar way. If temperature was excessively low, the distribution would be less uniform, and the composition would be extremely repetitive. On the other hand, as the temperature grows increasingly high, the the distribution would be more uniform, leading to increasingly less-similar but less coherent (in terms of aurally compelling "reproductions" of a given dataset).
                
                </p>
</p>
                </div>
            </div>
        </div>
    </section>



  <section id="services">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Project Report</h2>
                    <hr class="primary">
                    Our task is to create a machine composer that learn composer and genre style, and  produce original work in that style. This task is important because we can experiment with different genres or mixes of input data to create new styles and compositions that can inspire our own work, or possibly be quality compositions in their own right. Bach was only able to write 500 chorals in his lifetime. Wouldn’t it be great if we could do this in an hour?  Furthermore, by interacting with a machine composer, writers can be provided material and inspiration for their work.
                        <br><br>
                    Using Daniel Johnson's Biaxial Long Short-Term Memory Biaxial Recurrent Neural Network, we generated new compositions of music in a variety of styles, from Classical (e.g., music in the style of Fredyryk Chopin) to Classic Rock (e.g., Music in the style of beatles). This neural network learner's strengths included high quality outputs – as music was aesthetically pleasing and aurally resembled the genre of the training set – and comprehensive feature set, as pattern recognition, meter, pitch-class were some of the features used. Given the complexity of the learner required to perform automated music composition coupled with an in-depth analysis of Daniel Johnson's algorithm itself, we concluded that a neural network is the only learner capable of producing meaningful outputs in this discipline. Furthermore, in experimenting with hyperparamter adjustment – such as dropout and temeprature – we determined that the adjustments that led to the most aurally recognizable changes resided in adjusting the number of nodes of hidden layers across the time and note axes.
                </div>
            </div>
        </div>
    </section>




    <section class="no-padding" id="portfolio">
        <div class="container-fluid">
            <div class="row no-gutter popup-gallery">
                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/1.jpg" class="portfolio-box">
                        <img src="img/bach.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classical Music Weakness: Multitracking
                                </div>
                                <div class="project-name">
                                    Dataset: Bach Fugues and Chorales
                                </div>
                            </div>
                        </div>
                    </a>
                </div>
                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/2.jpg" class="portfolio-box">
                        <img src="img/multitrack.jpg" class="img-responsive" width="80%" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classical Music Strength: Single Track
                                </div>
                                <div class="project-name">
                                    Dataset: More Comprehensive Bach Dataset
                                </div>
                            </div>
                        </div>
                    </a>
                </div>
                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/3.jpg" class="portfolio-box">
                        <img src="img/chopin.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classical Music: Expanding Composer Breadth via Fryderyk Chopin
                                </div>
                                <div class="project-name">
                                    Can RNNs create distinctive styles across composers within genres?
                                </div>
                            </div>
                        </div>
                    </a>
                </div>

                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/4.jpg" class="portfolio-box">
                        <img src="img/beatles.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classic Rock: RNN and the Beatles
                                </div>
                                <div class="project-name">
                                    Dataset: 4 Albums of the Beatles
                                </div>
                            </div>
                        </div>
                    </a>
                </div>

                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/5.jpg" class="portfolio-box">
                        <img src="img/bruno.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Top 40 Hits meets RNN
                                </div>
                                <div class="project-name">
                                    Dataset: Compilation of Top 40 charts from past 10 years
                                </div>
                            </div>
                        </div>
                    </a>
                </div>

                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/6.jpg" class="portfolio-box">
                        <img src="img/house.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Who Needs Melody Anyways? A Rhythmic Approach to RNN
                                </div>
                                <div class="project-name">
                                    Dataset: Compilation of house and electronic music
                                </div>
                            </div>
                        </div>
                    </a>
                </div>
            </div>
        </div>
    </section>

    <aside class="bg-dark">
        <div class="container text-center">
            <div class="call-to-action">
                <h2>Want to Learn More?</h2>
                <a href="https://github.com/hexahedria/biaxial-rnn-music-composition" class="btn btn-default btn-xl sr-button">Visit Daniel Johnson's Page here!</a>
            </div>
        </div>
    </aside>

    <section id="contact">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Bibliography</h2>
                    <p align="left">Johnson, Daniel. "Composing Music With Recurrent Neural Networks." Hexahedria. August 02, 2015. Accessed May 30, 2017. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/.</p>

                    <h2 class="section-heading">Contact Us</h2>
                    <hr class="primary">
                    <p>Robbie, Ben, and Victor are undergraduate students at Northwestern University studying a variety of disciplines – from Electrical Engineering and Computer Science to Classical Guitar Performance and Music.</p>
                </div>
                <div class="col-lg-4 col-lg-offset-2 text-center">
                    <i class="fa fa-phone fa-3x sr-contact"></i>
                    <p>317-775-0018</p>
                </div>
                <div class="col-lg-4 text-center">
                    <i class="fa fa-envelope-o fa-3x sr-contact"></i>
                    <p><a href="mailto:your-email@your-domain.com">VictorLalo2017@u.northwestern.edu<br>
                    JohnKrege2018@u.northwestern.edu<br>
                    RobertBelson2019@u.northwestern.edu</a></p>
                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="vendor/scrollreveal/scrollreveal.min.js"></script>
    <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/creative.min.js"></script>

</body>

</html>
