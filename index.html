<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Ben Krege, Victor Lalo, Robbie Belson">

    <title>MusiqueMan</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <script src="index.js"></script>
    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/creative.min.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top">

<!-- <embed id="mus"
    src="homepage.mp3"
    loop="true"
    width="1"
    height="1"
    autostart="true"> -->

    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">MusiqueMan</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="#about">Project Overview</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#services">Approach and Design Considerations</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#final_report">Final Report</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <header>
        <div class="header-content">
            <div class="header-content-inner">
                <h1 id="homeHeading">Automated Music Generator using Recurrent Neural Network </h1>
                <hr>
                <p>EECS 349 Final Project <br>Ben Krege, Victor Lalo, and Robert Belson<br>
                    JohnKrege2018@u.northwestern.edu<br>
                    VictorLalo2017@u.northwestern.edu<br>
                    RobertBelson2019@u.northwestern.edu </p>
                <a href="#about" class="btn btn-primary btn-xl page-scroll">Learn More</a>
            </div>
        </div>
    </header>

    <section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Abstract</h2>
                    <hr class="light">
                    <p class="text-faded">Our task is to create a machine composer that learn composer and genre style, and  produce original work in that style. This task is important because we can experiment with different genres or mixes of input data to create new styles and compositions that can inspire our own work, or possibly be quality compositions in their own right. Bach was only able to write 500 chorals in his lifetime. Wouldn’t it be great if we could do this in an hour?  Furthermore, by interacting with a machine composer, writers can be provided material and inspiration for their work.
                        <br><br>
                    Using Daniel Johnson's Biaxial Long Short-Term Memory Biaxial Recurrent Neural Network, we generated new compositions of music in a variety of styles, from Classical (e.g., music in the style of Fredyryk Chopin) to Classic Rock (e.g., Music in the style of beatles). This neural network learner's strengths included high quality outputs – as music was aesthetically pleasing and aurally resembled the genre of the training set – and comprehensive feature set, as pattern recognition, meter, pitch-class were some of the features used. Given the complexity of the learner required to perform automated music composition coupled with an in-depth analysis of Daniel Johnson's algorithm itself, we concluded that a neural network is the only learner capable of producing meaningful outputs in this discipline. Furthermore, in experimenting with hyperparamter adjustment – such as dropout and temeprature – we determined that the adjustments that led to the most aurally recognizable changes resided in adjusting the number of nodes of hidden layers across the time and note axes.
</p>
                    <a href="#services" class="page-scroll btn btn-default btn-xl sr-button">How'd we do it?</a>
                </div>
            </div>
        </div>
    </section>

    <section id="services">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Project Design</h2>
                    <hr class="primary">
                </div>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-diamond text-primary sr-icons"></i>
                        <h3>Our Architecture: Daniel Johnson's Biaxial Long Short-Term Memory Recurrent Neural Network</h3>
                        <p class="text-muted">After just ~12 hours of training, producing a new piece takes just a matter of minutes. </p>
                    </div>
                </div>
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-paper-plane text-primary sr-icons"></i>
                        <h3>Endless, Recyclable Music Composition </h3>
                        <p class="text-muted">Using AWS to optimize performance, as the RNN was designed for GPUs, we queued up a variety of compositions over the course of a few weeks.</p>
                    </div>
                </div>
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-newspaper-o text-primary sr-icons"></i>
                        <h3>Our Data (MIDI Files)</h3>
                        <p class="text-muted">We have found numerous online MIDI libraries with raw data from a multitude of styles and composers. So far, we have used a dataset of a variety of works by Bach (chorales, fugues, variations, etc.) of size ~1.2MB (Complete Bach Midi Index from bachcentral.com). The 5 features include input position, pitch­ class, previous vicinity, previous context, and beat. As for examples, the examples are essential the repertoire itself, in which we will run out model on held out test data. Of the 1.2MB dataset we chose, all of it will be used for development/training, as a smaller partition may lead to an insufficient amount of data for automated music composition. </p>
                    </div>
                </div>
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-heart text-primary sr-icons"></i>
                        <h3>Breadth of Project</h3>
                        <p class="text-muted">Unlike Daniel Johnson, we sought to apply his neural network across a variety of styles, comparing its capcity to produce aurally compelling music. Beginning first with the classical genre, drawing from the compositions of Bach and Chopin, we then explored other genres such as classical rock, pop, and electronic and house music.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>



<section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">How does the Biaxial Recurrent Neural Network Work?</h2>
                    <hr class="light">
                    <p>Drawing from some concepts of convolution neural networks (applying the same invarient network on each time step)coupled with long short-term memory layers, our recurrent neural network learns to predict which notes will be played (among other paramters such as duration) at each time step (1/16th notes) of a piece.<br>

                    In basic recurrent neural networks, each output of a particular hidden layer, beyond the weighted sum of the inputs, is computed and fed back to itself, and other nodes in that "column," as an additional input in the next time step. Thus, each node of a hidden layer is derived by the outputs of the current layer and the inputs of the previous layer, as seen in the figure below:
                    <br>
                    <img src="img/rnn1.jpg" width="30%" align="middle" alt="">
                    <br>

                     For the network to have short-term-memory, an output needs to be an input in the next time step. Long Short-Term Memory (LSTM) nodes includes a saved value that is included in each node's calculations. This value can be modified (e.g., addeded, subtracted, etc.) at each time step.
                <br>
                <img src="img/rnn2.jpg" width="30%" align="middle" alt="">
                <br>
                    For this neural network adapted specifically to generate pieces of music, unique properties of this network included:
                <ol class="text-faded">
                  <li align="left">Note and Time Invariance: For the generated music to be transposable and composed to an indefinite length, the network was constructed to be identical for each note and time step (time+note invarient)</li>
                  <li align="left">Vertical and Horizontal Motion: Network supports polyphonic music, with multiple notes at a given time step</li>
                  <li align="left">Repetition as Integral Compositional Element: Western music is repetitive. Thus, the network must allow repetition of the same note. The uniqueness of this network is that it has an encoding for a note to be held. This contrasts some previous machine composer designs that did not distinguish between a held note and a repeated note.</li>
                </ol>
                However, to integrate pattern recognition over time (an integral component to constructing aurally). Coined as "Biaxial RNN" by Daniel Johnson, the network has time axis and note axis, connecting inputs to outputs and recurrent connections from one axis to another.

                REWRITE

                The first two layers default to 300 nodes each and feed back via the RNN through the time axis. The third and fourth timesteps are 200 and 50, respectively. feed back into themselves to give an awareness of harmony.

                <br>
                <br>
                <a id="rnncontbutt" class="page-scroll btn btn-default btn-xl sr-button">Implementation Details (Cont.)</a> 
</p>
                </div>
            </div>
        </div>
    </section>




<section hidden class="bg-primary" id="rnn_cont">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Deep Dive into Our Biaxial Recurrent Neural Network</h2>
                    <hr class="light">
                    <p align="left">For each time step we have the following paramters to our input vector:
                    <ol class="text-faded">
                  <li align="left">Position: Current pitch of a particular note, ranging from 0-127 (MIDI note value) </li>
                  <li align="left">Pitchlass: Consisting of 12 values, one per pitchclass, incrementing by 1 for each value (A will be 0, A# will be 1.... G# will be 11). In this assumption, Johnson posits that this will allow selection of "more common chords." However, this assumption is incredibly naive, given his assertion that is is necessarily "more common to have a C major chord than E-flat major chord." In certain iterations of our model, we adjusted this paramter to reflect a different weighting of commonality of various chords. </li>
                  <li align="left">Previous Vicinity: Consisting of 50 values, or rather 3 octaves span of 12 pitch classes, each value is given a 1 if that pitch was present in the last timestep and a zero otherwise. </li>
                  <li align="left">Previous Context: Similar to Previous Vicinity paramter, but condensed to 1 octave, number of times a certain pitch class was played across any octave is reported across the 12 values.</li>
                  <li align="left">Beat: Using a 4 row binary representation, one for each beat of the 4/4 time signature, each 1 represents the "note-on" MIDI property for each timestep.</li>
                </ol>
                <p align="left" >Now, to break down the two LSTM stacks:<br>
                First Hidden LSTM Stack: LSTMs that have reccurent connections along time axis, where the last layer outputs a note state representing any particular time pattern</p> <br>
                <p align="left" >Second Hidden LSTM Stack: LSTMs that have recurrent connections along note axis, whos last layer outputs 2 values. The first value, Play Probability, indicates the probability that a particular note should be chosen to be played. The second value, Articulate Probability (applies only for notes that have already been played in prior time step), is probability the note should last longer, say, from a sixteenth note to an eighth note. </p>
                <h2 class="section-heading">Model Software Implementation </h2>
                <p align="left" >Having studied a variety of machine learning software this quarter, the biaxial network uses Theano, a python library developed by computer scientists at Université de Montréal. This software compiles the network to run most efficiently by making the code GPU-optimized.
                <br>
                
                </p>
                <ol> Training Process Stepwise Process:
                  <li align="left">Randomly select batch of short music segments from our dataset, and feed into biaxial RNN</li>
                  <li align="left">Take output probabilities from second hidden LSTM stack, and calculate cross-entropy (i.e., likelihood of generating particular output) </li>
                  <li align="left">Take probabilities and use optimizer (AdaDelta) to calculate weights </li>
                  <li align="left">Batch all notes together and train time-axis layers</li>
                  <li align="left">Do the reverse: batch all times together and train the note-axis layers</li>
                </ol>
                <h2 class="section-heading">Avoiding Overfitting: Hyperparameter Adjustments </h2>
                <p align="left">Applying dropout to our RNN is a way of removing hidden nodes (by zeroing output) at each layer at random, as a means to avoid overfitting and in turn promote specializing, or rather "unique" music composition. <br>
                *Note: We would expect temperature adjustments to behave in a similar way. If temperature was excessively low, the distribution would be less uniform, and the composition would be extremely repetitive. On the other hand, as the temperature grows increasingly high, the the distribution would be more uniform, leading to increasingly less-similar but less coherent (in terms of aurally compelling "reproductions" of a given dataset).
                
                </p>
</p>
                </div>
            </div>
        </div>
    </section>



  <section id="final_report">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Project Report</h2>
                    <hr class="primary">
                    <p align="left">
                    From Google’s Magenta and Wavenet, to Stanford University's GRUV or Cambridge University’s BachBot, the plethora of deep learning software for music generation available today share one thing in common – its use of neural networks. One of the most compelling elements of a piece in music is the series of relationships constructed across the piece from note-to-note, harmony-to-harmony, or even larger macrostructural relationships. These relationships – and its resulting patterns and repetition – are essential to music composition, and neural networks are thus the most natural choice for a learner. In conducting this project, we looked for a series of other techniques we could use to generate music. One of the leading alternatives we considered was using a decision tree: in segmenting a dataset of thousands of harmonic progressions sorted in a CSV, a probabilistic model would help generate a new series of harmonies – and upon generating the new harmonic progression, we would use Python’s MIDI package to generate these harmonies. While this would have been a successful project, the generated harmonies would have no way to morph into a more elaborate piece of music. With neural networks, computation was done at the level of each individual notes, leading to a much wider range of compositions. Thus, we chose neural networks as our backend architecture for the project.<br><br>
                    For each output generated (see below), each training set was composed a series of MIDI files in that style or genre. For example, in our Bach dataset, we had approximately 1.2MB of a variety of Bach selections, from fugues to chorales. The specific features of our data, outlined in our “How does the Biaxial Recurrent Neural Network Work?” section above, is composed of:<br><br>
                    1. Pitch class:Current pitch of a particular note, ranging from the MIDI note values 0-127) <br>
                    2. Previous vicinity: A set of 50 values, representing the three nearest octaves of pitches, where each value is given a 1 if that pitch was present in the last timestep and a zero otherwise<br>
                    3. Previous Context: Similar to Previous Vicinity parameter, but condensed to 1 octave, number of times a certain pitch class was played across any octave is reported across the 12 values <br> 
                    4. Beat: Using a 4 row binary representation, one for each beat of the 4/4 time signature, each 1 represents the "note-on" MIDI property for each timestep.<br><br>
                    
                    <h1 font-weight="bold">The Datasets</h1><br>
                    <p align="left">
                    We used four datasets for our project. Each set contained between 1 and 2 MB of data.<br><br>

                    Bach 1.0: Our First Bach Dataset: In this dataset, we used approximately ~1.2MB of a variety of Bach selections, from fugues to chorales. Characteristics of this dataset include standard harmonic progressions of Western Tonal music (i.e., what is often referred to as “progressive harmony”), multiple independent voices, and modulations to closely related keys (e.g., C Major to G Major). The MIDI was represented in multi-track form, and the RNN tried to reconcile chord structures and melodies across tracks, a feat significantly more challenging than if all tracks (voices) were condensed into one. As a result, the output resulted in a fugue-esque composition, with two independent melodic lines at a particularly slow tempo in C Major. Especially after hearing the audio output, this led us to conclude that a relative weakness of Daniel Johnson’s algorithm was multitrack MIDI. In future iterations (i.e., Bach 2.0), we used single-track MIDI. <br><br>
                    
                    <audio controls>
                      <source src="Bach_FULL_No_FX.mp3" type="audio/mpeg">
                    </audio>
                    <br>

                    Bach 2.0: More Comprehensive Bach Dataset: As expected, this dataset ran significantly better than our first Bach dataset. This was due to changing the MIDI Format (see MIDI Formatting section) from 1 to 0, as well as almost doubling the amount of training examples given. We ran 153 Bach compositions, with MIDI note counts ranging from 312 notes to 2,930 notes, averaging at around 1,044 notes per file. As a result, the composition features a clear melodic line with a supporting accompaniment figure in Eb Major. This significantly more harmonically nuanced composition supported our earlier hypothesis that the RNN architecture prefers single track MIDI.<br><br>

                    <audio controls>
                      <source src="Bad_Bach_FULL_FX.mp3" type="audio/mpeg">
                    </audio>
                    <br>
                    The perplexity and error of the final training epoch was:
                    <code>epoch 9900, minute 24939121.4239, error=2092.07885742, perplexity=0.000772314505144</code><br><br>


                    Chopin Dataset: We ran 95 Chopin compositions, with MIDI note counts ranging from 675 notes to 4,827 notes, averaging at around 1,662 notes per file. Inspired by Polish folk tunes, characteristic of Chopin compositions was melodic ornamentation, increasingly complex modulations, and more liberal tempos. In the Chopin output, the output resembled many of his nocturnes, with a slightly syncopated rhythm in Eb Major. While the composition was by no means a “perfect replica” of a Chopin composition, many of Chopin’s idiosyncratic compositional elements (i.e., chromatic passing tones in melodic line, general tonal areas that were less likely to be in C Major) were certainly present.<br><br>

                     <audio controls>
                      <source src="Chopin_FULL_AllFX.mp3" type="audio/mpeg">
                    </audio>
                    <br>
                    The perplexity and error of the final training epoch was:
                    <code>epoch 9800, minute 24940234.9389, error=3977.01831055, perplexity=0.000845743685353</code><br><br>


                    Beatles Dataset: The Beatles have a genre-defying style, and their musical energy range exceeds any other pop group. Their compositions include slow ballads, upbeat rock, lullabies, acoustic folk, and psychedelic drones. It is very difficult to pinpoint a specific mood, structure, or even melodic pattern since the quartet changed their sound so drastically from one album to the next. We grabbed every song from seven of their studio albums to analyze: Rubber Soul, Revolver, Sgt. Pepper’s, Magical Mystery Tour, The White Album, Abbey Road, and Let it Be, along with some single releases.In total, we ran 96 Beatles compositions, with MIDI note counts ranging from 1,762 notes to 10,988 notes, averaging at around 3,762 notes per file. The amount of notes played in Beatles music was much higher on average than that of classical compositions. Our guess is this has to do with the instrumentation of Beatles songs, having many more voices complementing each other at once. We believe this allowed the neural network to pick up on rhythmic patterns very well.<br>

                     <audio controls>
                      <source src="Beatles_ChordProg_FX.mp3" type="audio/mpeg">
                    </audio>
                    <br>
                    The perplexity and error of the final training epoch was:
                    <code>epoch 9800, minute 24941982.3987, error=2119.68139648, perplexity=0.000781532744972</code><br><br>

                     <br>The software’s output really impressed us. Although we mentioned above that it is difficult to define the Beatles’ musical style, we could make out much of Paul McCartney and John Lennon’s songwriting in the neural net’s creation. From the beginning of the piece, I noticed the algorithm was able to recognize a rhythmic pattern used a lot in McCartney’s piano playing. It captures the chords repeating on the upbeat, while the bass plays on the downbeat. The alternating style, found in many Beatles’ tracks, captures the listener with differing frequency ranges responding to each other over time. This section’s melody also complements Lennon’s descending scale tones, used in many of his vocal lines (e.g. Blue Jay Way, Come Together). At around 1 minute in, we start to hear the use of an alternating root bass against elegant and complex major chords, with an uplifting melody, very reminiscent of some of McCartney’s sillier or lighter songs (e.g. Maxwell’s Silver Hammer, Hello Goodbye, Good Day Sunshine). The chords contain many dominant seventh and major seventh notes, a key component of multiple Beatles’ progressions. At around the 1:35 mark, we hear a beautifully crafted progression in A minor, which many people would recognize as a standard pop progression used by many top artists today. It amazes us that the neural network is able to implicitly learn, to some extent, what makes chords fit well together. The higher pitched melody gracefully dances above the chords and almost seamlessly blends back into the chords themselves. This is our favorite section of this track for it’s simple, yet almost human feel. It reminds us of McCartney’s solo piano songs (e.g. Hey Jude, Golden Slumbers), delicately fusing vocals and piano parts into very emotionally charged compositions.</p><br><br>

                    <pre>&#9</pre> 
                    Input Formatting: While gathering MIDI files for analysis, we found that most notation divides a piece into different tracks to represent separate voicings. This makes sense for playing back a composition with multiple instruments, but for our analysis, we wanted all parts of the piece (melody, chords, bass) on a single piano track, allowing the algorithm to focus on a single file per piece. While researching how MIDI files are written, we learned that there are multiple MIDI formats. Most of our examples are MIDI Format 1, which are files that contain multiple tracks for output on multiple instruments. We therefore used a simple conversion software called Sweet Midi Converter to turn these files into MIDI Format 0, which unifies all tracks into one.<br>
                    <br>Another problem we ran into was rhythm sections. Percussion in terms of MIDI relies on using hit samples placed into a sampler. A sampler still uses a note convention, assigning a sample to each key on a keyboard, but when read by a note analyzer, it throws off the rest of the song’s harmony components. In other words, if the song is in the scale of C Major, and the sampler has a kick sample on B2 and a snare sample on D#3, feeding this track to our algorithm would greatly alter the conclusion of harmony drawn. Our solution was to remove all percussion tracks from the tracks. Bach and Chopin pieces had very little change, since they are meant to be played as solo or duet pianos. Our Beatles dataset on the other hand had percussion on almost every file. Midi Format 1 worked in our favor here as we were able to delete the individual percussion tracks and then save the file without them. We then finally converted the now percussion-less files into Format 0 for a single MIDI track.<br><br>

                    <pre>&#9</pre> 
                    Output Modulation/Manipulation: In order to turn the neural net’s MIDI output into useful audio, we imported the files into Logic Pro X, processing and editing within the Digital Audio Workstation for a more pleasant listening experience. For comparison, we have included the original neural network’s output alongside our edited versions. In our opinion, the edited versions have more “feel” and the listener can relate more to this experience.<br><br>

                    <pre>&#9</pre> 
                    MIDI Effects:  In terms of the MIDI notes, we wanted to give our machine’s performance a human feel. We achived this with several effects that manipulate the note’s timing, velocity, and relative pitch. The first addition was adding subtle swing to our pieces. Swing moves the notes very slightly off of the grid, giving the performance a less than perfect timing. Human error is part of many individual styles, and sounding mechanically perfect detracts from a piece’s movement. The Beatles output received the biggest timing change, while the Bach output received minimal swing, accounting for style each composer was intending to play. Next we modulated the note’s velocities. Again, having every note hit at the same force constantly detracts from a song’s dynamic energy. The velocity modulator allows for a random velocity (0-127 value) within a specified range and with a bias towards playing harder or softer more often. Lastly, we used a scale transposer to keep the song harmonious, avoiding dissonant notes. We set a scale, say C Major, and if Logic detects a note that is not within the scale it transposes it to the closest note that is in the scale. This is surely the most controversial edit we made to our neural nets output, but we see it as adding an extra constraint on the output. By filtering incorrect notes, the piece sounds more cohesive and allows a listener to better follow along. The classical compositions tended to lean more towards major scales, while The Beatles had simpler, although more minor tonalities.
                     <img src="img/swing_and_quantization.png" width="60%" align="middle" alt="">
                     <img src="img/velocity_and_scale.png" width="60%" align="middle" alt="">
                     <pre>&#9</pre> 
                    Audio Output: As for the actual auditory output, we ran our MIDI notes through a sampler loaded with recordings of individual Boesendorfer Grand Piano keys, each with multiple velocities. This is as close as we can get to replicating an acoustic piano. Aside from the instrument, we used an equalizer to shape the tone and a compressor for dynamic stability. Lastly, we placed convolutional reverb on our audio chain, giving each output a unique space. The reverb works by grabbing frequency responses of specific rooms/halls. Each piece’s sound is slightly different than the others to account for the style of the composer. We believe this instrumentation assists in expressing what the network has learned and created.

                    <pre>&#9</pre> 
                    Future Works: Very pleased with the work and outputs of the project, there are many areas for further exploration. Firstly, we could experiment with a variety of other different musical genres (i.e., pop, blues, jazz, etc.), creating datasets on which to run our RNN. Additionally, another significantly time-consuming project could be to modify the algorithm to support a wider range of rhythmic devices (e.g., 32nd notes), and harmonic devices (e.g., create different weightings of “vicinity” feature to support more interesting melodic lines for, say, 12-tone compositions).</p> 

                    <pre>&#9</pre> 
                    <h1>Group Responsibilities<h1>
                    <p>Ben: Backend infrastructure work with AWS <br>
                    Victor: Dataset manipulation and FX synthesis <br>
                    Robbie: Front-end webpage design, harmonic analyses of RNN outputs<p>






                </div>
            </div>
        </div>
    </section>




   <!--  <section class="no-padding" id="portfolio">
        <div class="container-fluid">
            <div class="row no-gutter popup-gallery">
                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/1.jpg" class="portfolio-box">
                        <img src="img/bach.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classical Music Weakness: Multitracking
                                </div>
                                <div class="project-name">
                                    Dataset: Bach Fugues and Chorales
                                </div>
                            </div>
                        </div>
                    </a>
                </div>
                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/2.jpg" class="portfolio-box">
                        <img src="img/multitrack.jpg" class="img-responsive" width="80%" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classical Music Strength: Single Track
                                </div>
                                <div class="project-name">
                                    Dataset: More Comprehensive Bach Dataset
                                </div>
                            </div>
                        </div>
                    </a>
                </div>
                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/3.jpg" class="portfolio-box">
                        <img src="img/chopin.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classical Music: Expanding Composer Breadth via Fryderyk Chopin
                                </div>
                                <div class="project-name">
                                    Can RNNs create distinctive styles across composers within genres?
                                </div>
                            </div>
                        </div>
                    </a>
                </div>

                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/4.jpg" class="portfolio-box">
                        <img src="img/beatles.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Classic Rock: RNN and the Beatles
                                </div>
                                <div class="project-name">
                                    Dataset: 4 Albums of the Beatles
                                </div>
                            </div>
                        </div>
                    </a>
                </div>

                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/5.jpg" class="portfolio-box">
                        <img src="img/bruno.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Top 40 Hits meets RNN
                                </div>
                                <div class="project-name">
                                    Dataset: Compilation of Top 40 charts from past 10 years
                                </div>
                            </div>
                        </div>
                    </a>
                </div>

                <div class="col-lg-4 col-sm-6">
                    <a href="img/portfolio/fullsize/6.jpg" class="portfolio-box">
                        <img src="img/house.jpg" width="80%" class="img-responsive" alt="">
                        <div class="portfolio-box-caption">
                            <div class="portfolio-box-caption-content">
                                <div class="project-category text-faded">
                                    Who Needs Melody Anyways? A Rhythmic Approach to RNN
                                </div>
                                <div class="project-name">
                                    Dataset: Compilation of house and electronic music
                                </div>
                            </div>
                        </div>
                    </a>
                </div>
            </div>
        </div>
    </section> -->

    <aside class="bg-dark">
        <div class="container text-center">
            <div class="call-to-action">
                <h2>Want to Learn More?</h2>
                <a href="https://github.com/hexahedria/biaxial-rnn-music-composition" class="btn btn-default btn-xl sr-button">Visit Daniel Johnson's Page here!</a>
            </div>
        </div>
    </aside>

    <section id="contact">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Bibliography</h2>
                    <p align="left">Johnson, Daniel. "Composing Music With Recurrent Neural Networks." Hexahedria. August 02, 2015. Accessed May 30, 2017. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/.</p>

                    <h2 class="section-heading">Contact Us</h2>
                    <hr class="primary">
                    <p>Robbie, Ben, and Victor are undergraduate students at Northwestern University studying a variety of disciplines – from Electrical Engineering and Computer Science to Classical Guitar Performance and Music.</p>
                </div>
                <div class="col-lg-4 col-lg-offset-2 text-center">
                    <i class="fa fa-phone fa-3x sr-contact"></i>
                    <p>317-775-0018</p>
                </div>
                <div class="col-lg-4 text-center">
                    <i class="fa fa-envelope-o fa-3x sr-contact"></i>
                    <p><a href="mailto:your-email@your-domain.com">VictorLalo2017@u.northwestern.edu<br>
                    JohnKrege2018@u.northwestern.edu<br>
                    RobertBelson2019@u.northwestern.edu</a></p>
                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="vendor/scrollreveal/scrollreveal.min.js"></script>
    <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/creative.min.js"></script>

</body>

</html>
